{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoOShGJt4FCc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53a9bc0c-a20d-4b63-b36c-dcb50674af05"
      },
      "source": [
        "# Start by importing all the things we'll need.\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, CuDNNLSTM, Flatten, TimeDistributed, Dropout, LSTMCell, RNN, Bidirectional, Concatenate, Layer\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.python.keras.utils import tf_utils\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os \n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.14.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVf5ETRY5P9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the english text corpus\n",
        "file = open(\"./data.txt\", 'r')\n",
        "corpus = [line for line in file]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWRrtdr-5-qm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "3d0ffc8b-d11a-453f-db6a-4e41c6db1cea"
      },
      "source": [
        "# Sample sentences\n",
        "corpus[0:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\ufeffHey, place the toolbox in the truck!\\n',\n",
              " '\\n',\n",
              " 'Hey, what are you doing here?\\n',\n",
              " '\\n',\n",
              " 'Hey, give me a break.\\n',\n",
              " '\\n',\n",
              " 'Hey, John.\\n',\n",
              " '\\n',\n",
              " \"Hey, what are you doing? Don't touch it, or you'll break the balance!\\n\",\n",
              " '\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4RDSM7D6IM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing punctuation from the sentences\n",
        "def clean_special_chars(text, punct):\n",
        "    for p in punct:\n",
        "        text = text.replace(p, '')\n",
        "    return text\n",
        "\n",
        "# Calling function to remove the punctuation and turning data into lower case      \n",
        "def preprocess(data):\n",
        "    output = []\n",
        "    punct = '#$%&*+-/<=>@[\\\\]^_`{|}~\\t\\n'\n",
        "    for line in data:\n",
        "         pline= clean_special_chars(line.lower(), punct)\n",
        "         output.append(pline)\n",
        "    return output  \n",
        "\n",
        "# Convert the sentence to char gram\n",
        "def generate_dataset():\n",
        "    processed_corpus = preprocess(corpus)    \n",
        "    output = []\n",
        "    for line in processed_corpus:\n",
        "        token_list = line\n",
        "        for i in range(1, len(token_list)):\n",
        "            data = []\n",
        "            x_ngram = '<start> '+ token_list[:i+1] + ' <end>'\n",
        "            y_ngram = '<start> '+ token_list[i+1:] + ' <end>'\n",
        "            data.append(x_ngram)\n",
        "            data.append(y_ngram)\n",
        "            output.append(data)\n",
        "    print(\"Dataset prepared with prefix and suffixes for teacher forcing technique\")\n",
        "    dummy_df = pd.DataFrame(output, columns=['input','output'])\n",
        "    return output, dummy_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lad4AY2u6Mo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LanguageIndex():\n",
        "    def __init__(self, lang):\n",
        "        self.lang = lang\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.vocab = set()\n",
        "        self.create_index()\n",
        "    def create_index(self):\n",
        "        for phrase in self.lang:\n",
        "            self.vocab.update(phrase.split(' '))\n",
        "        self.vocab = sorted(self.vocab)\n",
        "        self.word2idx[\"<pad>\"] = 0\n",
        "        self.idx2word[0] = \"<pad>\"\n",
        "        for i,word in enumerate(self.vocab):\n",
        "            self.word2idx[word] = i + 1\n",
        "            self.idx2word[i+1] = word\n",
        "\n",
        "def max_length(t):\n",
        "    return max(len(i) for i in t)\n",
        "\n",
        "def load_dataset():\n",
        "    pairs,df = generate_dataset()\n",
        "    out_lang = LanguageIndex(sp for en, sp in pairs)\n",
        "    in_lang = LanguageIndex(en for en, sp in pairs)\n",
        "    input_data = [[in_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n",
        "    output_data = [[out_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n",
        "\n",
        "    max_length_in, max_length_out = max_length(input_data), max_length(output_data)\n",
        "    input_data = tf.keras.preprocessing.sequence.pad_sequences(input_data, maxlen=max_length_in, padding=\"post\")\n",
        "    output_data = tf.keras.preprocessing.sequence.pad_sequences(output_data, maxlen=max_length_out, padding=\"post\")\n",
        "    return input_data, output_data, in_lang, out_lang, max_length_in, max_length_out, df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I-E5t6K6Je2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e5286eb2-0af4-4c89-beef-8b60891d638c"
      },
      "source": [
        "# Preprocess the prepare the data\n",
        "input_data, teacher_data, input_lang, target_lang, len_input, len_target, df = load_dataset()\n",
        "\n",
        "\n",
        "target_data = [[teacher_data[n][i+1] for i in range(len(teacher_data[n])-1)] for n in range(len(teacher_data))]\n",
        "target_data = tf.keras.preprocessing.sequence.pad_sequences(target_data, maxlen=len_target, padding=\"post\")\n",
        "target_data = target_data.reshape((target_data.shape[0], target_data.shape[1], 1))\n",
        "\n",
        "# Shuffle all of the data in unison. This training set has the longest (e.g. most complicated) data at the end,\n",
        "# so a simple Keras validation split will be problematic if not shuffled.\n",
        "\n",
        "p = np.random.permutation(len(input_data))\n",
        "input_data = input_data[p]\n",
        "teacher_data = teacher_data[p]\n",
        "target_data = target_data[p]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset prepared with prefix and suffixes for teacher forcing technique\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEXrNmEv6S-H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0f9fb99f-57a2-4910-add6-93a08c8ffed8"
      },
      "source": [
        "# Set the parameter to train the model\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "BUFFER_SIZE = len(input_data)\n",
        "BATCH_SIZE = 128\n",
        "embedding_dim = 300\n",
        "units = 128\n",
        "vocab_in_size = len(input_lang.word2idx)\n",
        "vocab_out_size = len(target_lang.word2idx)\n",
        "df.iloc[60:65]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>&lt;start&gt; hey, what are you doing he &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; re? &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>&lt;start&gt; hey, what are you doing her &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; e? &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>&lt;start&gt; hey, what are you doing here &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; ? &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>&lt;start&gt; hey, what are you doing here? &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt;  &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>&lt;start&gt; he &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; y, give me a break. &lt;end&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          input                             output\n",
              "60  <start> hey, what are you doing he <end>     <start> re? <end>                \n",
              "61  <start> hey, what are you doing her <end>    <start> e? <end>                 \n",
              "62  <start> hey, what are you doing here <end>   <start> ? <end>                  \n",
              "63  <start> hey, what are you doing here? <end>  <start>  <end>                   \n",
              "64  <start> he <end>                             <start> y, give me a break. <end>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M3leLTd6dYY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "af96e8e2-d536-4699-ef48-8f6e53f4260d"
      },
      "source": [
        "# Create the Encoder layers first.\n",
        "encoder_inputs = Input(shape=(len_input,))\n",
        "encoder_emb = Embedding(input_dim=vocab_in_size, output_dim=embedding_dim)\n",
        "\n",
        "# Use this if you dont need Bidirectional LSTM\n",
        "# encoder_lstm = CuDNNLSTM(units=units, return_sequences=True, return_state=True)\n",
        "# encoder_out, state_h, state_c = encoder_lstm(encoder_emb(encoder_inputs))\n",
        "\n",
        "encoder_lstm = Bidirectional(CuDNNLSTM(units=units, return_sequences=True, return_state=True))\n",
        "encoder_out, fstate_h, fstate_c, bstate_h, bstate_c = encoder_lstm(encoder_emb(encoder_inputs))\n",
        "\n",
        "state_h = Concatenate()([fstate_h,bstate_h])\n",
        "state_c = Concatenate()([bstate_h,bstate_c])\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "\n",
        "# Now create the Decoder layers.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_emb = Embedding(input_dim=vocab_out_size, output_dim=embedding_dim)\n",
        "decoder_lstm = CuDNNLSTM(units=units*2, return_sequences=True, return_state=True)\n",
        "decoder_lstm_out, _, _ = decoder_lstm(decoder_emb(decoder_inputs), initial_state=encoder_states)\n",
        "# Two dense layers added to this model to improve inference capabilities.\n",
        "decoder_d1 = Dense(units, activation=\"relu\")\n",
        "decoder_d2 = Dense(vocab_out_size, activation=\"softmax\")\n",
        "decoder_out = decoder_d2(Dropout(rate=.2)(decoder_d1(Dropout(rate=.2)(decoder_lstm_out))))\n",
        "\n",
        "\n",
        "# Finally, create a training model which combines the encoder and the decoder.\n",
        "# Note that this model has three inputs:\n",
        "model = Model(inputs = [encoder_inputs, decoder_inputs], outputs= decoder_out)\n",
        "\n",
        "# We'll use sparse_categorical_crossentropy so we don't have to expand decoder_out into a massive one-hot array.\n",
        "# Adam is used because it's, well, the best.\n",
        "\n",
        "model.compile(optimizer=tf.train.AdamOptimizer(), loss=\"sparse_categorical_crossentropy\", metrics=['sparse_categorical_accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_23\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_23 (InputLayer)           [(None, 113)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_22 (Embedding)        (None, 113, 300)     1168800     input_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_24 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_11 (Bidirectional [(None, 113, 256), ( 440320      embedding_22[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "embedding_23 (Embedding)        (None, None, 300)    1288200     input_24[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 256)          0           bidirectional_11[0][1]           \n",
            "                                                                 bidirectional_11[0][3]           \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 256)          0           bidirectional_11[0][3]           \n",
            "                                                                 bidirectional_11[0][4]           \n",
            "__________________________________________________________________________________________________\n",
            "cu_dnnlstm_23 (CuDNNLSTM)       [(None, None, 256),  571392      embedding_23[0][0]               \n",
            "                                                                 concatenate_22[0][0]             \n",
            "                                                                 concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, None, 256)    0           cu_dnnlstm_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_22 (Dense)                (None, None, 128)    32896       dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, None, 128)    0           dense_22[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_23 (Dense)                (None, None, 4294)   553926      dropout_22[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 4,055,534\n",
            "Trainable params: 4,055,534\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QDj2JCD6hWU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "754a7d7f-e865-40b7-98ff-2f519af273e1"
      },
      "source": [
        "# Note, we use 20% of our data for validation.\n",
        "epochs = 10\n",
        "history = model.fit([input_data, teacher_data], target_data,\n",
        "                 batch_size= BATCH_SIZE,\n",
        "                 epochs=epochs,\n",
        "                 validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 30010 samples, validate on 7503 samples\n",
            "Epoch 1/10\n",
            "30010/30010 [==============================] - 41s 1ms/sample - loss: 0.7521 - sparse_categorical_accuracy: 0.9382 - val_loss: 0.3241 - val_sparse_categorical_accuracy: 0.9475\n",
            "Epoch 2/10\n",
            "30010/30010 [==============================] - 40s 1ms/sample - loss: 0.2960 - sparse_categorical_accuracy: 0.9496 - val_loss: 0.2655 - val_sparse_categorical_accuracy: 0.9518\n",
            "Epoch 3/10\n",
            "30010/30010 [==============================] - 40s 1ms/sample - loss: 0.2372 - sparse_categorical_accuracy: 0.9554 - val_loss: 0.2028 - val_sparse_categorical_accuracy: 0.9633\n",
            "Epoch 4/10\n",
            "30010/30010 [==============================] - 40s 1ms/sample - loss: 0.1880 - sparse_categorical_accuracy: 0.9638 - val_loss: 0.1576 - val_sparse_categorical_accuracy: 0.9718\n",
            "Epoch 5/10\n",
            "30010/30010 [==============================] - 40s 1ms/sample - loss: 0.1521 - sparse_categorical_accuracy: 0.9706 - val_loss: 0.1280 - val_sparse_categorical_accuracy: 0.9767\n",
            "Epoch 6/10\n",
            "30010/30010 [==============================] - 40s 1ms/sample - loss: 0.1267 - sparse_categorical_accuracy: 0.9754 - val_loss: 0.1085 - val_sparse_categorical_accuracy: 0.9803\n",
            "Epoch 7/10\n",
            "30010/30010 [==============================] - 40s 1ms/sample - loss: 0.1090 - sparse_categorical_accuracy: 0.9785 - val_loss: 0.0954 - val_sparse_categorical_accuracy: 0.9822\n",
            "Epoch 8/10\n",
            "30010/30010 [==============================] - 40s 1ms/sample - loss: 0.0955 - sparse_categorical_accuracy: 0.9808 - val_loss: 0.0848 - val_sparse_categorical_accuracy: 0.9840\n",
            "Epoch 9/10\n",
            "30010/30010 [==============================] - 40s 1ms/sample - loss: 0.0846 - sparse_categorical_accuracy: 0.9827 - val_loss: 0.0764 - val_sparse_categorical_accuracy: 0.9855\n",
            "Epoch 10/10\n",
            "30010/30010 [==============================] - 40s 1ms/sample - loss: 0.0758 - sparse_categorical_accuracy: 0.9843 - val_loss: 0.0702 - val_sparse_categorical_accuracy: 0.9867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb5fD5yI7aH5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "6940a83a-d3db-4ef5-d0c7-56d03f6f1c5a"
      },
      "source": [
        "# Plot the results of the training.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label=\"Training loss\")\n",
        "plt.plot(history.history['val_loss'], label=\"Validation loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBKRwFxFJQAp3EZEEpHAXEUlA/x/bgNJJCN0c8AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg5twhoi6sCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the encoder model from the tensors we previously declared.\n",
        "encoder_model = Model(encoder_inputs, [encoder_out, state_h, state_c])\n",
        "\n",
        "# Generate a new set of tensors for our new inference decoder. Note that we are using new tensors, \n",
        "# this does not preclude using the same underlying layers that we trained on. (e.g. weights/biases).\n",
        "\n",
        "inf_decoder_inputs = Input(shape=(None,), name=\"inf_decoder_inputs\")\n",
        "# We'll need to force feed the two state variables into the decoder each step.\n",
        "state_input_h = Input(shape=(units*2,), name=\"state_input_h\")\n",
        "state_input_c = Input(shape=(units*2,), name=\"state_input_c\")\n",
        "decoder_res, decoder_h, decoder_c = decoder_lstm(\n",
        "    decoder_emb(inf_decoder_inputs), \n",
        "    initial_state=[state_input_h, state_input_c])\n",
        "inf_decoder_out = decoder_d2(decoder_d1(decoder_res))\n",
        "inf_model = Model(inputs=[inf_decoder_inputs, state_input_h, state_input_c], \n",
        "                  outputs=[inf_decoder_out, decoder_h, decoder_c])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7LMjd9K7gen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converts the given sentence (just a string) into a vector of word IDs\n",
        "# Output is 1-D: [timesteps/words]\n",
        "\n",
        "def sentence_to_vector(sentence, lang):\n",
        "\n",
        "    pre = sentence\n",
        "    vec = np.zeros(len_input)\n",
        "    sentence_list = [lang.word2idx[s] for s in pre.split(' ')]\n",
        "    for i,w in enumerate(sentence_list):\n",
        "        vec[i] = w\n",
        "    return vec\n",
        "\n",
        "# Given an input string, an encoder model (infenc_model) and a decoder model (infmodel),\n",
        "def translate(input_sentence, infenc_model, infmodel):\n",
        "    sv = sentence_to_vector(input_sentence, input_lang)\n",
        "    sv = sv.reshape(1,len(sv))\n",
        "    [emb_out, sh, sc] = infenc_model.predict(x=sv)\n",
        "    \n",
        "    i = 0\n",
        "    start_vec = target_lang.word2idx[\"<start>\"]\n",
        "    stop_vec = target_lang.word2idx[\"<end>\"]\n",
        "    \n",
        "    cur_vec = np.zeros((1,1))\n",
        "    cur_vec[0,0] = start_vec\n",
        "    cur_word = \"<start>\"\n",
        "    output_sentence = \"\"\n",
        "\n",
        "    while cur_word != \"<end>\" and i < (len_target-1):\n",
        "        i += 1\n",
        "        if cur_word != \"<start>\":\n",
        "            output_sentence = output_sentence + \" \" + cur_word\n",
        "        x_in = [cur_vec, sh, sc]\n",
        "        [nvec, sh, sc] = infmodel.predict(x=x_in)\n",
        "        cur_vec[0,0] = np.argmax(nvec[0,0])\n",
        "        cur_word = target_lang.idx2word[np.argmax(nvec[0,0])]\n",
        "    return output_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCx_pqq27lJb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1fc0f402-d200-4e28-b196-33a4c97cb2b1"
      },
      "source": [
        "#Note that only words that we've trained the model on will be available, otherwise you'll get an error.\n",
        "\n",
        "\n",
        "test = [\n",
        "    'What are',\n",
        "    'Thank you',\n",
        "    'Have a',\n",
        "    'See',\n",
        "    'How old',\n",
        "]\n",
        "  \n",
        "\n",
        "import pandas as pd\n",
        "output = []  \n",
        "for t in test:  \n",
        "  output.append({\"Input seq\":t.lower(), \"Pred. Seq\":translate(t.lower(), encoder_model, inf_model)})\n",
        "\n",
        "results_df = pd.DataFrame.from_dict(output) \n",
        "results_df.head(len(test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Input seq</th>\n",
              "      <th>Pred. Seq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>what are</td>\n",
              "      <td>you going?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>thank you</td>\n",
              "      <td>you for the consideration you will give to our request.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>have a</td>\n",
              "      <td>wonderful eating experience.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>see</td>\n",
              "      <td>you next week.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>how old</td>\n",
              "      <td>are you doing? it's been a long time since i've seen you.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Input seq                                                    Pred. Seq\n",
              "0  what are     you going?                                               \n",
              "1  thank you    you for the consideration you will give to our request.  \n",
              "2  have a       wonderful eating experience.                             \n",
              "3  see          you next week.                                           \n",
              "4  how old      are you doing? it's been a long time since i've seen you."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    }
  ]
}